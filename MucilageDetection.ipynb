{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MucilageDetection.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"E3sewH60IJBF"},"source":["To work on colab, we need to add some paths and install libraries that are already installed on local computer. This part is not needed to run on local machine.\n","\n"]},{"cell_type":"code","metadata":{"id":"i9OPLHIBWCdd"},"source":["!pip install patchify\n","!pip install dropbox\n","!pip install gputil\n","!pip install psutil\n","!pip install humanize"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PXacKu0nWGr5"},"source":["We need to test the given GPU statistics"]},{"cell_type":"code","metadata":{"id":"LtWNhEn6WMKd"},"source":["# Import packages\n","import os,sys,humanize,psutil,GPUtil\n","\n","# Define function\n","def mem_report():\n","  print(\"CPU RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ))\n","  \n","  GPUs = GPUtil.getGPUs()\n","  for i, gpu in enumerate(GPUs):\n","    print('GPU {:d} ... Mem Free: {:.0f}MB / {:.0f}MB | Utilization {:3.0f}%'.format(i, gpu.memoryFree, gpu.memoryTotal, gpu.memoryUtil*100))\n","    \n","# Execute function\n","mem_report()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nBPu5RjTCsQw"},"source":["## Define Paths and Folders\n","\n","Define the paths and folders for the training and testing."]},{"cell_type":"code","metadata":{"id":"bNYK4jgNCZii"},"source":["import os\n","dropboxFolderPath = 'E:/Dropbox/'\n","\n","# define the paths relative to the dropbox folder\n","unetWorkingPath = os.path.join(dropboxFolderPath, 'Education/PhD/Projects/MucilageDetection/uNetLearning')\n","\n","sentinelTestImagePath = '/Dataset/satellite/sentinel2/35TPE_MATDATA/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"11Zhmbjbo_Rp"},"source":["import sys\n","from google.colab import drive\n","\n","# mount the drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# override the local paths\n","unetWorkingPath = '/content/drive/Othercomputers/My Laptop/uNetLearning/'\n","\n","# add the paths to system paths \n","sys.path.append(unetWorkingPath)\n","sys.path.append(os.path.join(unetWorkingPath, 'functions'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j090V7UhIYIu"},"source":["# Prepare For Train and Test\n","\n","Training code has been written in python. We use uNet architecture to segment the mucilage from the water. The implementation fulfilled with pyTorch. *italicized text*"]},{"cell_type":"code","metadata":{"id":"RSLXRmQIUC3J"},"source":["# define common paths\n","modelSavePath = os.path.join(unetWorkingPath, 'models')\n","sentinelTrainImagePath = os.path.join(unetWorkingPath, 'patches')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6POCWujxItXo"},"source":["## Import Libraries\n","\n","To utilize training, we need to import necessary libraries. Some of the libraries are the standart pyTorch libarries and these can be imported via colab. The custom libraries are imported via the google drive."]},{"cell_type":"code","metadata":{"id":"s_X5MT2RIrZK"},"source":["# these are the standart libraries\n","import os\n","import torch\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","from torch.utils.data import DataLoader\n","from torchsummary import summary\n","\n","# these are the custom libraries and will be imported from the drive\n","from unet.unet_model import UNet\n","from functions.ModelTrainer import train_model\n","from functions.SentinelLoader import SentinelPatchLoader\n","from functions.DataTransformer import GetDataTransformer"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w-aVaKZGHMXO"},"source":["## Create Device"]},{"cell_type":"code","metadata":{"id":"ExZz6gLzHLCW"},"source":["# create pytorch device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f'Training device is {device}')\n","\n","# clear GPU memory\n","if device == 'cuda':\n","  torch.cuda.empty_cache()\n","  torch.cuda.clear_cache()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T-M57LdGI8gy"},"source":["## Define Optional Settings\n","\n","We need to set the necessary parameters for training. The path for the training and validation patches are automatically selected as local path or drive path.\n","\n","batchSize: set the mini batch size for the training\n","patchSize: number of patches that are cropped from the training images\n","loadAllAtOnce: load all data to memory before running, this will increase the training process but memory could be insufficient"]},{"cell_type":"code","metadata":{"id":"1wzCksKhJAc9"},"source":["# options\n","patchSize = 192\n","batchSize = 64\n","patchCount = 200\n","loadAllAtOnce = True"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TxePWbSyHBvt"},"source":["## Train and Validation Dataset"]},{"cell_type":"code","metadata":{"id":"7i8V6CFCJPaA"},"source":["# print the location of patches\n","print(f'Sentinel Patches will be used from {sentinelTrainImagePath}')\n","\n","# define train images\n","TrainingImages = [\"S2A_MSIL2A_20210402T085551_N0300_R007_T35TPF_20210402T133128\",\n","                  \"S2A_MSIL2A_20210509T084601_N0300_R107_T35TPF_20210509T115513\",\n","                  \"S2A_MSIL2A_20210512T085601_N0300_R007_T35TPF_20210512T133202\"]\n","TrainingDataLoader = SentinelPatchLoader(sentinelTrainImagePath, TrainingImages, patchCount, loadAllAtOnce, GetDataTransformer())\n","\n","# define validation images\n","ValidationImages = [\"S2B_MSIL2A_20210414T084559_N0300_R107_T35TPF_20210414T112733\"]\n","ValidationDataLoader = SentinelPatchLoader(sentinelTrainImagePath, ValidationImages, patchCount, loadAllAtOnce, GetDataTransformer())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q1HF5DxDXVcb"},"source":["# create custom data loader\n","TrainingDataLoader = {\n","    'train': DataLoader(TrainingDataLoader, batch_size=batchSize, shuffle=True, num_workers=0),\n","    'val': DataLoader(ValidationDataLoader, batch_size=batchSize, shuffle=True, num_workers=0)\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KUGOzvPoJVkb"},"source":["## Create Model\n","\n","Create the uNet model which has 9 channel input and 1 semantic class. After creating the model we will try to import the previous best weights for initialization. If the previous training run interrupted, code will try to recover from the last known state by loading the trainedModel. "]},{"cell_type":"code","metadata":{"id":"PWeVg0XlIGbg"},"source":["# create a ResNetUNet model and apply it to model\n","trainModel = UNet(n_channels=9, n_classes=1)\n","trainModel = trainModel.to(device)\n","\n","# print the model summary\n","summary(trainModel, (9, patchSize, patchSize))\n","\n","# define optimizer and scheduler\n","optimizer = optim.Adam(filter(lambda p: p.requires_grad, trainModel.parameters()), lr=0.0005)\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.5)\n","\n","# check for previous model\n","lastTrainedModelPath = os.path.join(modelSavePath, 'trainedModel')\n","preTrainedModelPath = os.path.join(modelSavePath,'bestModel')\n","if os.path.isfile(lastTrainedModelPath):\n","    print('loading the previous model...')\n","    \n","    # load the model\n","    checkpoint = torch.load(lastTrainedModelPath)\n","    trainModel.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    epoch = checkpoint['epoch']\n","    loss = checkpoint['loss']\n","elif os.path.isfile(preTrainedModelPath):\n","    print('using the pre-trained model weights...')\n","    weights = torch.load(preTrainedModelPath)\n","    trainModel.load_state_dict(weights)\n","else:\n","    print('no previous model found, training from scratch')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2WBeO7R4JXTG"},"source":["# Training Code\n","\n","Training code has been written in python. We use uNet architecture to segment the mucilage from the water. The implementation fulfilled with pyTorch."]},{"cell_type":"markdown","metadata":{"id":"kqfE4saUJMtc"},"source":["## Define Train and Test Set\n","\n","In this part we define **train** and **validation** set. Since we label only 4 images, we use 3 of them for training and 1 of them for validation. \n","\n","*Note that this part will take some time if the loadAllAtOnce flag set to True, but it will speed up the traing*"]},{"cell_type":"code","metadata":{"id":"4A4TKlhUJYjX"},"source":["# start training\n","print('training model...')\n","train_model(trainModel, optimizer, exp_lr_scheduler, TrainingDataLoader, device, num_epochs=200, outputPath=modelSavePath)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zg28-ixTAt6F"},"source":["# Test Model\n","\n","In this section we are going to use the model trained in the previous section and generate the output for different images."]},{"cell_type":"code","metadata":{"id":"nhrQpdXlVEJS"},"source":["TestingImages = {\n","    \"S2A_MSIL2A_20210102T090351_N0214_R007_T35TPE_20210102T115327_20m.mat\",\n","    \"S2A_MSIL2A_20210119T085301_N0214_R107_T35TPE_20210119T115427_20m.mat\",\n","    \"S2A_MSIL2A_20210129T085221_N0214_R107_T35TPE_20210129T103011_20m.mat\",\n","    \"S2A_MSIL2A_20210221T090001_N0214_R007_T35TPE_20210221T115031_20m.mat\",\n","    \"S2A_MSIL2A_20210303T085851_N0214_R007_T35TPE_20210303T105606_20m.mat\",\n","    \"S2A_MSIL2A_20210313T085741_N0214_R007_T35TPE_20210313T112414_20m.mat\",\n","    \"S2A_MSIL2A_20210402T085551_N0300_R007_T35TPE_20210402T133128_20m.mat\",\n","    \"S2A_MSIL2A_20210422T085551_N0300_R007_T35TPE_20210422T122938_20m.mat\",\n","    \"S2A_MSIL2A_20210429T084601_N0300_R107_T35TPE_20210429T103912_20m.mat\",\n","    \"S2A_MSIL2A_20210509T084601_N0300_R107_T35TPE_20210509T115513_20m.mat\",\n","    \"S2A_MSIL2A_20210512T085601_N0300_R007_T35TPE_20210512T133202_20m.mat\",\n","    \"S2A_MSIL2A_20210519T084601_N0300_R107_T35TPE_20210519T115101_20m.mat\",\n","    \"S2A_MSIL2A_20210519T084601_N0300_R107_T35TPF_20210519T115101_20m.mat\",\n","    \"S2A_MSIL2A_20210611T085601_N0300_R007_T35TPE_20210611T121904_20m.mat\",\n","    \"S2A_MSIL2A_20210618T084601_N0300_R107_T35TPE_20210618T120513_20m.mat\",\n","    \"S2A_MSIL2A_20210628T084601_N0300_R107_T35TPE_20210628T103709_20m.mat\",\n","    \"S2A_MSIL2A_20210701T085601_N0301_R007_T35TPE_20210701T125029_20m.mat\",\n","    \"S2A_MSIL2A_20210711T085601_N0301_R007_T35TPE_20210711T121659_20m.mat\",\n","    \"S2A_MSIL2A_20210718T084601_N0301_R107_T35TPE_20210718T114908_20m.mat\",\n","    \"S2A_MSIL2A_20210728T084601_N0301_R107_T35TPE_20210728T111949_20m.mat\",\n","    \"S2A_MSIL2A_20210731T085601_N0301_R007_T35TPE_20210731T120834_20m.mat\",\n","    \"S2A_MSIL2A_20210807T084601_N0301_R107_T35TPE_20210807T114132_20m.mat\",\n","    \"S2A_MSIL2A_20210810T085601_N0301_R007_T35TPE_20210810T120517_20m.mat\",\n","    \"S2A_MSIL2A_20210817T084601_N0301_R107_T35TPE_20210817T114824_20m.mat\",\n","    \"S2A_MSIL2A_20210820T085601_N0301_R007_T35TPE_20210820T122214_20m.mat\",\n","    \"S2A_MSIL2A_20210827T084601_N0301_R107_T35TPE_20210827T102802_20m.mat\",\n","    \"S2A_MSIL2A_20210830T085601_N0301_R007_T35TPE_20210831T164139_20m.mat\",\n","    \"S2B_MSIL2A_20210203T085059_N0214_R107_T35TPE_20210203T111110_20m.mat\",\n","    \"S2B_MSIL2A_20210206T090039_N0214_R007_T35TPE_20210206T112619_20m.mat\",\n","    \"S2B_MSIL2A_20210223T084849_N0214_R107_T35TPE_20210223T111955_20m.mat\",\n","    \"S2B_MSIL2A_20210226T085829_N0214_R007_T35TPE_20210226T112525_20m.mat\",\n","    \"S2B_MSIL2A_20210305T084739_N0214_R107_T35TPE_20210305T111205_20m.mat\",\n","    \"S2B_MSIL2A_20210308T085719_N0214_R007_T35TPE_20210308T113808_20m.mat\",\n","    \"S2B_MSIL2A_20210328T085559_N0214_R007_T35TPE_20210328T113525_20m.mat\",\n","    \"S2B_MSIL2A_20210407T085549_N0300_R007_T35TPE_20210407T123314_20m.mat\",\n","    \"S2B_MSIL2A_20210414T084559_N0300_R107_T35TPE_20210414T112733_20m.mat\",\n","    \"S2B_MSIL2A_20210427T085549_N0300_R007_T35TPE_20210427T113845_20m.mat\",\n","    \"S2B_MSIL2A_20210507T085559_N0300_R007_T35TPE_20210507T192808_20m.mat\",\n","    \"S2B_MSIL2A_20210514T084559_N0300_R107_T35TPE_20210514T113538_20m.mat\",\n","    \"S2B_MSIL2A_20210517T085559_N0300_R007_T35TPE_20210517T112912_20m.mat\",\n","    \"S2B_MSIL2A_20210517T085559_N0300_R007_T35TPF_20210517T112912_20m.mat\",\n","    \"S2B_MSIL2A_20210524T084559_N0300_R107_T35TPE_20210524T111238_20m.mat\",\n","    \"S2B_MSIL2A_20210606T085559_N0300_R007_T35TPE_20210606T120423_20m.mat\",\n","    \"S2B_MSIL2A_20210613T084559_N0300_R107_T35TPE_20210613T113603_20m.mat\",\n","    \"S2B_MSIL2A_20210623T084559_N0300_R107_T35TPE_20210623T112759_20m.mat\",\n","    \"S2B_MSIL2A_20210626T085559_N0300_R007_T35TPE_20210626T114028_20m.mat\",\n","    \"S2B_MSIL2A_20210713T084559_N0301_R107_T35TPE_20210713T115731_20m.mat\",\n","    \"S2B_MSIL2A_20210716T085559_N0301_R007_T35TPE_20210716T112700_20m.mat\",\n","    \"S2B_MSIL2A_20210726T085559_N0301_R007_T35TPE_20210726T122123_20m.mat\",\n","    \"S2B_MSIL2A_20210802T084559_N0301_R107_T35TPE_20210802T110537_20m.mat\",\n","    \"S2B_MSIL2A_20210805T085559_N0301_R007_T35TPE_20210805T113028_20m.mat\",\n","    \"S2B_MSIL2A_20210812T084559_N0301_R107_T35TPE_20210812T105857_20m.mat\",\n","    \"S2B_MSIL2A_20210815T085559_N0301_R007_T35TPE_20210815T111606_20m.mat\",\n","    \"S2B_MSIL2A_20210822T084559_N0301_R107_T35TPE_20210822T110737_20m.mat\",\n","    \"S2B_MSIL2A_20210825T085559_N0301_R007_T35TPE_20210825T120947_20m.mat\",\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MSnCB-mfaTsR"},"source":["## Options\n","\n","Set the test options\n","\n","- modelResolution is needed for output array\n","- testbatchSize can be larger than the trainBatchSize\n","- cropZone is used to reduce tested patch size"]},{"cell_type":"code","metadata":{"id":"gzcYU5tbVBKO"},"source":["testBatchSize = 64\n","cropZone = (0,0, 4223, 1727)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1bfkqr9cZzQu"},"source":["## Create Model and Load the Best Weights\n","\n","Create the same model as in training phase and load the model parameters from the training."]},{"cell_type":"code","metadata":{"id":"VXMBxfo7e65U"},"source":["# include test related libraries\n","import numpy as np\n","import torch.nn.functional as func\n","import scipy.io as sio\n","\n","from functions.SentinelLoader import SentinelTestDataset\n","from functions.TestImagePathFinder import GetTestImagePath"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TtAlQGVr_y-Z"},"source":["testModel = UNet(n_channels=9, n_classes=1)\n","testModel = testModel.to(device)\n","print('loading pretrained model...')\n","testModel.load_state_dict(torch.load(os.path.join(modelSavePath, 'bestModel')))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N_3f1NLmZyJz"},"source":["# create output directory\n","outputDirectory = os.path.join(unetWorkingPath, \"outputs\")\n","if not os.path.exists(outputDirectory):\n","    os.makedirs(outputDirectory)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aS6gW-nhaCZH"},"source":["# test the images one by one and save the result\n","for TestImage in TestingImages:\n","    \n","    # get the path to the image\n","    ImagePath = GetTestImagePath(dropboxFolderPath, sentinelTestImagePath, TestImage)\n","\n","    # create loader\n","    TestingDataLoader = SentinelTestDataset(ImagePath, cropZone, patchSize, GetDataTransformer())\n","    \n","    # load the patches with batchSize\n","    TestDataLoader = {\n","        'test': DataLoader(TestingDataLoader, batch_size=testBatchSize, shuffle=False, num_workers=0)\n","    }\n","    \n","    # create output patches\n","    patchCountX = (cropZone[2] - cropZone[0]) // patchSize\n","    patchCountY = (cropZone[3] - cropZone[1]) // patchSize\n","    result = np.zeros((patchCountY,patchCountX,1,patchSize,patchSize), dtype=np.float32)\n","    \n","    # find the results\n","    for inputs, rows, cols in TestDataLoader['test']:\n","        inputs = inputs.to(device)\n","    \n","        # get the result\n","        with torch.set_grad_enabled(False):\n","            outputs = func.sigmoid(testModel(inputs)).contiguous()\n","    \n","        # convert tensor to np array\n","        outputNP = outputs.cpu().detach().numpy()\n","        rowsNP = rows.cpu().detach().numpy()\n","        colsNP = cols.cpu().detach().numpy()\n","        \n","        # add the result into the big array\n","        for i in range(outputNP.shape[0]):\n","            result[rowsNP[i],colsNP[i],:,:,:] = outputNP[i,:,:,:]\n","        \n","    # convert patches to image\n","    OutputFileName = os.path.splitext(TestImage)[0]+'_MUCILAGE.mat'\n","    sio.savemat(os.path.join(outputDirectory, OutputFileName), {'mucilage': result})"],"execution_count":null,"outputs":[]}]}